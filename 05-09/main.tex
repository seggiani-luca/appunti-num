
\documentclass[a4paper,11pt]{article}
\usepackage[a4paper, margin=8em]{geometry}

% usa i pacchetti per la scrittura in italiano
\usepackage[french,italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\frenchspacing 

% usa i pacchetti per la formattazione matematica
\usepackage{amsmath, amssymb, amsthm, amsfonts}

% usa altri pacchetti
\usepackage{gensymb}
\usepackage{hyperref}
\usepackage{standalone}

% imposta il titolo
\title{Appunti Calcolo Numerico}
\author{Luca Seggiani}
\date{2025}

% disegni
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}

% imposta lo stile
% usa helvetica
\usepackage[scaled]{helvet}
% usa palatino
\usepackage{palatino}
% usa un font monospazio guardabile
\usepackage{lmodern}

% tikz in sans
\tikzset{every picture/.style={/utils/exec={\sffamily}}}

\renewcommand{\rmdefault}{ppl}
\renewcommand{\sfdefault}{phv}
\renewcommand{\ttdefault}{lmtt}

% circuiti
\usepackage{circuitikz}
\usetikzlibrary{babel}

% disponi il titolo
\makeatletter
\renewcommand{\maketitle} {
	\begin{center} 
		\begin{minipage}[t]{.8\textwidth}
			\textsf{\huge\bfseries \@title} 
		\end{minipage}%
		\begin{minipage}[t]{.2\textwidth}
			\raggedleft \vspace{-1.65em}
			\textsf{\small \@author} \vfill
			\textsf{\small \@date}
		\end{minipage}
		\par
	\end{center}

	\thispagestyle{empty}
	\pagestyle{fancy}
}
\makeatother

% disponi teoremi
\usepackage{tcolorbox}
\newtcolorbox[auto counter, number within=section]{theorem}[2][]{%
	colback=blue!10, 
	colframe=blue!40!black, 
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries, 
	title=Teorema~\thetcbcounter: #2, 
	#1
}

% disponi definizioni
\newtcolorbox[auto counter, number within=section]{definition}[2][]{%
	colback=red!10,
	colframe=red!40!black,
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries,
	title=Definizione~\thetcbcounter: #2,
	#1
}

% disponi problemi
\newtcolorbox[auto counter, number within=section]{problem}[2][]{%
	colback=green!10,
	colframe=green!40!black,
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries,
	title=Problema~\thetcbcounter: #2,
	#1
}

% disponi codice
\usepackage{listings}
\usepackage[table]{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{codestyle}{
	backgroundcolor=\color{black!5}, 
	commentstyle=\color{codegreen},
	keywordstyle=\bfseries\color{magenta},
	numberstyle=\sffamily\tiny\color{black!60},
	stringstyle=\color{green!50!black},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstdefinestyle{shellstyle}{
	backgroundcolor=\color{black!5}, 
	basicstyle=\ttfamily\footnotesize\color{black}, 
	commentstyle=\color{black}, 
	keywordstyle=\color{black},
	numberstyle=\color{black!5},
	stringstyle=\color{black}, 
	showspaces=false,
	showstringspaces=false, 
	showtabs=false, 
	tabsize=2, 
	numbers=none, 
	breaklines=true
}

\lstdefinelanguage{javascript}{
	keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
	keywordstyle=\color{blue}\bfseries,
	ndkeywords={class, export, boolean, throw, implements, import, this},
	ndkeywordstyle=\color{darkgray}\bfseries,
	identifierstyle=\color{black},
	sensitive=false,
	comment=[l]{//},
	morecomment=[s]{/*}{*/},
	commentstyle=\color{purple}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	morestring=[b]',
	morestring=[b]"
}

% disponi sezioni
\usepackage{titlesec}

\titleformat{\section}
{\sffamily\Large\bfseries} 
{\thesection}{1em}{} 
\titleformat{\subsection}
{\sffamily\large\bfseries}   
{\thesubsection}{1em}{} 
\titleformat{\subsubsection}
{\sffamily\normalsize\bfseries} 
{\thesubsubsection}{1em}{}

% disponi alberi
\usepackage{forest}

\forestset{
	rectstyle/.style={
		for tree={rectangle,draw,font=\large\sffamily}
	},
	roundstyle/.style={
		for tree={circle,draw,font=\large}
	}
}

% disponi algoritmi
\usepackage{algorithm}
\usepackage{algorithmic}
\makeatletter
\renewcommand{\ALG@name}{Algoritmo}
\makeatother

% disponi numeri di pagina
\usepackage{fancyhdr}
\fancyhf{} 
\fancyfoot[L]{\sffamily{\thepage}}

\makeatletter
\fancyhead[L]{\raisebox{1ex}[0pt][0pt]{\sffamily{\@title \ \@date}}} 
\fancyhead[R]{\raisebox{1ex}[0pt][0pt]{\sffamily{\@author}}}
\makeatother

\begin{document}

% sezione (data)
\section{Lezione del 09-05-25}

% stili pagina
\thispagestyle{empty}
\pagestyle{fancy}

% testo
\subsection{Approssimazione di equazioni non lineari}
Vogliamo risolvere equazioni del tipo:
$$
f(x) = 0, \quad f : \mathbb{R} \rightarrow \mathbb{R}
$$
con $f$ non lineare ($f(x) \neq ax + b$).

Osserviamo che in generale non c'è un'espressione analitica, quindi una formula chiusa, per tutti i punti $\alpha \in \mathbb{R}$ tali che $f(\alpha) = 0$, quindi che risolvono l'equazione.

Diamo quindi la definizione elementare:
\begin{definition}{Radice}
	Per un sistema $f(x) = 0$, $\alpha \in \mathbb{R}$ tale che $f(\alpha) = 0$ è detta radice o zero di $f$.
\end{definition}

Vorremo quindi cercare delle approssimazioni di $\alpha$, e più precisamente vogliamo considerare metodi numerici che generano successioni $\{x_n\}_{n \in \mathbb{N}}$ che sperabilmente hanno la proprietà:
$$
\lim_{n \rightarrow + \infty} x_n = \alpha
$$
In particolare vedremo metodi del tipo:
$$
x_{n + 1} = \phi_n (x_n, x_{n - 1}, ..., x_{n - k + 1})
$$

La funzione $\phi_n$ accetta $k$ argomenti (punti) e viene detta \textbf{funzione di iterazione}.
Se $\phi_n = \phi$, $\forall n \in \mathbb{N}$ parliamo di \textbf{metodi stazionari}.

Diamo quindi la definizione di \textbf{convergenza}:
\begin{definition}{Convergenza}
	Un metodo iterativo per risolvere $f(x)$ si dice convergente per $k$ punti iniziali $x_0, x_{-1}, ..., x_{1-k}$ se la successione generata:
	$$
	x_{n + 1} = \phi_n (x_n, x_{n - 1}, ..., x_{n - k + 1})
	$$
	verifica:
	$$
	\lim_{n \rightarrow + \infty} x_n = \alpha
	$$
	con $\alpha$ radice di $f$.
\end{definition}

Possiamo generalizzare l'idea di convergenza a convergenza \textit{di un certo ordine}:
\begin{definition}{Ordine di convergenza}
	Si dice che un metodo iterativo ha convergenza di ordine $p \geq 1$ se $\exists c < +\infty$ (costante finita), $c \neq 0$ tale per cui:
	$$
	\lim_{n \rightarrow +\infty} \frac{|x_{n + 1} - \alpha|}{|x_n - \alpha|^p} = c
	$$
	dove nel caso $p = 1$ si richiede anche $0 < c < 1$.
\end{definition}

Osserviamo quindi che la definizione dice che, asintoticamente, si ha $|e_{n + 1}| \approx |e_n|^p$.
Da questo è chiaro che se $e_{n}$ è un numero piccolo ($|e_n| \in (0, 1)$), per $p \geq 2$ (o $p = 1$ con $0 < c < 1$, come richiesto dalla definizione) $e_{n + 1}$ sarà ancora più piccolo.

In particolare si ha quindi che:
\begin{itemize}
	\item Per $p = 1$ si parla di convergenza \textbf{lineare}, cioè l'errore si riduce di un fattore $c$ ad ogni passo;
	\item Per $p = 2$ si parla di convergenza \textbf{quadratica}, e via dicendo.
\end{itemize}

Possiamo parlare di \textbf{convergenza locale}:
\begin{definition}{Convergenza locale}
	Si cha un metodo converge localmente (con ordine $p$) se $\exists S \subset \mathbb{R}$, con $\alpha \in S$, tale che per ogni scelta di $x_0, ..., x_{1 - k} \in S$, $x_n$ converge ad $\alpha$ (con ordine $p$).
\end{definition}

\subsubsection{Metodi grafici}
Riassumendo, si ha quindi che è importante saper localizzare/stimare dove sono le radici di una certa $f$ di cui ci interessa $f(x) = 0$.
Per questo scopo si possono usare strumenti di analisi del grafico di una funzione (almeno nel caso scalare):
\begin{enumerate}
	\item Se $f$ continua e $f(a) \cdot f(b) < 0 \implies \exists$ almeno una radice in $[a ,b]$ (che potrebbe essere più di una).
	\item Si può studiare la derivata, ammesso $f \in C'([a, b])$, e quindi del segno di $f$.

		\noindent
		\textbf{\textsf{Esempio}}

		Poniamo ad esempio di avere:
		$$
		f(x) = x \log(x) + \frac{1}{3}
		$$
		e di porci la domanda di trovare le radici approssimate di $f(x)$.
		Innanzitutto restringiamo il dominio a $x > 0$, e prendiamo i limiti agli estremi:
		$$
		\lim_{x \rightarrow 0^+} x \log(x) + \frac{1}{3} = \frac{1}{3}, \quad
		\lim_{x \rightarrow +\infty} x \log(x) + \frac{1}{3} = +\infty
		$$

		Prendiamo quindi la derivata:
		$$
		f'(x) = \log(x) + 1 \implies
		\begin{cases}
			f'(x) \geq 0, \quad x \geq \frac{1}{e} \\
			f'(x) \leq 0, \quad 0 < x \leq \frac{1}{e}
		\end{cases}
		$$
		e quindi $f'\left( \frac{1}{e} \right) = 0$, e la funzione descresce in $\left(0, \frac{1}{e} \right)$ per poi crescere in $\left( \frac{1}{e}, +\infty \right)$.

		Calcolando $f$ in $\frac{1}{e}$ si ha:
		$$
		f\left( \frac{1}{e} \right) = \frac{1}{e} \log\left( \frac{1}{e} \right) + \frac{1}{3} = \frac{1}{3} - \frac{1}{e} < 0
		$$
		per cui si avranno necessariamente due radici, comprese, nelle regioni:
		$$
		\alpha_1 \in \left(0, \frac{1}{e} \right), \quad \alpha_2 \in \left( \frac{1}{e}, +\infty \right)
		$$

	\item Si può procedere per separazione grafica.
		Data:
		$$
			f(x) = g(x) - h(x)
		$$
		dove $g(x)$ e $h(x)$ hano grafico noto, può essere conveniente sfruttare:
		$$
		f(x) = 0 \, \Leftrightarrow \, g(x) = h(x)
		$$
		cioè cercare i punti di intersezione fra i grafici di $g$ e $h$.
		
		\noindent
		\textbf{\textsf{Esempio}}
		
		Poniamo di avere:
		$$
		f(x) = 5x^2 - 2 e^x
		$$
		cioè:
		\[
			\begin{cases}
				h(x) = 2 e^x \\
				g(x) = 5 x^2
			\end{cases}
		\]	

		A sinistra del grafico, cioè per $\mathbb{R}^-$, ci sarà chiaramente una qualche soluzione $\alpha_1$.
		
		A destra del grafico si ha invece che $e^x$ va ad infinito più velocemente di $e^x$, cioè i grafici possono:
		\begin{enumerate}
			\item O non toccarsi mai;
			\item O toccarsi due volte (con $e^x$ che interseca $x^2$ in una fase iniziale dove va più lentamente, e quindi lo interseca di nuovo quando lo vince);
			\item O toccarsi una volta sola, come caso critico del caso precedente.
		\end{enumerate}

		Se quindi troviamo un punto $\tilde{x} \in (0, +\infty)$ dove $2e^{\tilde{x}} < 5 \tilde{x}^2$, siamo nel caso 2.
		Prendiamo allora $\tilde{x} = 2$, per cui:
		\[
			\begin{cases}
				g(2) = 20 \\
				h(2) = 2 e^2 = 2 e^2 < 2 \cdot 3^2 = 18
			\end{cases}
		\]
		per cui chiaramente avremo tre radici:
		$$
			\alpha_1 \in (-\infty, 0), \quad \alpha_2 \in (0, 2), \quad \alpha_3 \in (2, +\infty)
		$$

\end{enumerate}

\par\medskip

Iniziamo quindi a vedere i metodi iterativi veri e propri.
\subsubsection{Metodo di bisezione}
Il metodo di \textbf{bisezione} o \textit{ricerca binaria} (anche \textit{ricerca dicotomica}) consiste nel prendere un intervallo $[a, b]$ tale che $f(a) \cdot f(b) < 0$ (come nel primo esempio grafico della scorsa sezione).

In questo caso si prende come prima approssimazione:
$$
x_1 = \frac{x_0 + x_{-1}}{b}, \quad x_0 = a, \quad x_{-1} = b
$$

Si valuta quindi il segno di $f(x_1)$.
Se $f(x_1) \cdot f(x_0) < 0$, allora si scarta $b$ e si riparte con l'intervallo $[x_0, x_1]$, altrimenti si scarta $x_0$ e si riparte con l'intervallo $[x_1, x_{-1}]$.

Questo procedimento chiaramente è iterabile, e converge eventualmente ad \textit{una} radice $\alpha$, che non è detta essere l'unica.

La formula che si ottiene è la seguente:
$$
x_{n + 1} = \frac{x_{n - 1} + x_n}{2} 
$$

Per quanto riguarda l'errore (al caso \textit{idealmente} pessimo), abbiamo che questo si dimezza ad ogni passaggio:
$$
| x_{n + 1} - \alpha| < \frac{b - a}{2^n} \implies \lim_{n \rightarrow +\infty} |x_n - \alpha| = 0
$$

Questa stima è la migliore che possiamo dare, in quanto la funzione di errore $x_{n + 1} - \alpha$ non è monotona.
In ogni caso, può esserci utile a limitare l'errore con un numero minimo di passaggi al caso peggiore.

Infatti, se vogliamo:
$$
|x_n - \alpha| < t_{ol}
$$
dobbiamo usare $n$ iterazioni, con $n$ che verifica:
$$
\frac{b - a}{2^n} < t_{ol} \implies n > \log_2 \left( \frac{b - a}{t_{ol}} \right) \implies n = \left\lceil \log_2 \left( \frac{b - a}{t_{ol}} \right) \right\rceil
$$
approssimato al primo naturale superiore.

Ad esempio, se $b - a = 1$, servono $n = 10$ passi per ottenere $t_{ol} = 10^{-3}$, $n = 20$ per $t_{ol} = 10^{-6}$, e quindi in genere serviranno un numero sempre maggiore di misurazioni per ottenere precisioni migliori.

\subsubsection{Implementazione MATLAB del metodo di bisezione}
Vediamo l'implementazione MATLAB del metodo di bisezione:
\lstset{style=codestyle, language=MATLAB}
\lstinputlisting{../code/matlab/bisect.m}

Dove notiamo di aver adottato il criterio di stop:
$$
| x_{k + 1} - x_k | < t_{ol}
$$
per una tolleranza $t_{ol}$, di default $0.1$.

Inoltre, notiamo che per la valutazione del segno di $f(m)$, teniamo conto del segno di $f(a)$, in quanto vogliamo mantenere la radice sempre nell'intervallo $[a, b]$.

\par\smallskip

Vediamo ad esempio come applicare questo metodo per trovare una radice della funzione:
$$
f(x) = \left(x-3\right)^{4}-2
$$
sull'intervallo $[a, b] = [3, 5]$, con una differenza di step fra iterazioni al minimo di $0.01$.

In MATLAB definiremo $f$ come una function handle:
\begin{lstlisting}[language=MATLAB, style=codestyle]	
>> f = @(x) (x - 3) .^ 4 - 2

f =

  function_handle with value:

    @(x)(x-3).^4-2
\end{lstlisting}
e quindi applicheremo l'algoritmo appena definito:
\begin{lstlisting}[language=MATLAB, style=codestyle]	
>> bisect(f, 3, 5)

Iteration 1
	a: 3.0000
	b: 5.0000

Iteration 2
	a: 4.0000
	b: 5.0000

Iteration 3
	a: 4.0000
	b: 4.5000

Iteration 4
	a: 4.0000
	b: 4.2500

Iteration 5
	a: 4.1250
	b: 4.2500

Iteration 6
	a: 4.1875
	b: 4.2500

Iteration 7
	a: 4.1875
	b: 4.2188

Iteration 8
	a: 4.1875
	b: 4.2031

ans =

    4.1953
\end{lstlisting}

Si ottiene quindi $4.1953$, che per $f(4.1953) \approx 0.04$ è abbastanza vicino considerata la differenza di step minima imposta.
Concedendo all'algoritmo più passaggi, si otterrano chiaramente soluzioni via via più accurate.

\subsubsection{Metodo delle secanti}
Il metodo delle secanti si basa su un idea geometrica, cioè quella di tracciare la \textit{retta} secante di $f$ fra $a$ e $b$, ovvero quella passante per i punti $(a, f(a))$, $(b, f(b))$.
Di questa si trova quindi l'intersezione con l'asse delle ascisse, che chiamiamo ad esempio $x_1$:
$$
x_1 = x_b - f(x_b) \cdot \frac{x_b - x_a}{f(b) - f(x_{a})}
$$
A questo punto basterà iterare finche il punto di intersezione non è abbastanza vicino ad $\alpha$.

La formula che si ottiene è la seguente:
$$
x_{n + 1} = x_n - f(x_n) \cdot \frac{x_n - x_{n - 1}}{f(x_n) - f(x_{n - 1})}
$$

Vale riguardo all'errore il seguente teorema:
\begin{theorem}{Errore del metodo delle secanti}
	Se $f \in C^2([a, b])$ ammette radice, allora il metodo converge  localmente con ordine:
	$$
		p = \frac{1 + \sqrt{5}}{2} > 1
	$$
\end{theorem}

Questo solitamente risulta in un errore migliore del lineare ma peggiore del quadratico.

\subsubsection{Implementazione MATLAB del metodo delle secanti}
Vediamo quindi un implementazione MATLAB del metodo delle secanti scalare, per function handle:
\lstinputlisting{../code/matlab/secant_method.m}

Dove si usa lo stesso criterio di stop di prima.

\par\smallskip

Vediamo l'algoritmo in azione per la stessa funzione dell'esmpio 19.1.3:
\begin{lstlisting}[language=MATLAB, style=codestyle]	
>> secant_method(f, 3, 5, 100, 0.001) % step piu' piccolo

Iteration 1
	a: 3.0000
	b: 5.0000

Iteration 2
	a: 5.0000
	b: 3.2500

Iteration 3
	a: 3.2500
	b: 3.4684

Iteration 4
	a: 3.4684
	b: 13.1076

Iteration 5
	a: 13.1076
	b: 3.4702

[...]

Iteration 23
	a: 4.4099
	b: 4.1135

Iteration 24
	a: 4.1135
	b: 4.1703

Iteration 25
	a: 4.1703
	b: 4.1911

Iteration 26
	a: 4.1911
	b: 4.1892

ans =

    4.1892
\end{lstlisting}
dove si è ridotto lo step per far fronte alle fluttuazioni che il metodo incontra attorno alla radice, per via della derivata particolarmente "piatta" che $f(x)$ ha vicino a 3.
In particolare, si noti il salto che viene effettuato fra le iterazioni 4 e 5, che porta il valore da $\sim 3$ a $\sim 13$, cosa che chiaramente richiede alcuni passaggi per venire corretta.

\subsection{Metodi stazionari a un punto}
I \textbf{metodi stazionari a un punto} sono metodi della forma:
\[
	\begin{cases}
		x_0, \text{ dato o generato casualmente} \\
		x_{n + 1} = \phi(x_n)
	\end{cases}
\]

In questo il metodo di Jacobi e di Gauss-Seidel sono metodi stazionari a un punto.

Il modo in cui si sceglie $phi$ è (in analogia ai metodi per sistemi lineari) una funzione che verifica:
$$
\phi(\alpha) = \alpha, \quad \forall \alpha : f(\alpha) = 0
$$

Un modo canonico per costruire $\phi$ con questa proprietà è considerare:
$$
\phi(x) = x - g(x) f(x)
$$
con $g(x)$ tale che $g(x) \neq 0$ "vicino" ad $\alpha$.

Si vede che:
$$
\phi(\alpha) = \alpha - g(\alpha) f(\alpha) = \alpha
$$
e chiaramente il fatto che $g(\alpha) \neq 0$ indica che:
$$
\alpha \text{ punto fisso di $\phi$} \, \Leftrightarrow \, \alpha \text{ radice di $f$}
$$

Potremmo quindi chiederci quando $\{ \phi(x_n) \}_{n \in \mathbb{N}}$ è convergente.
Sfruttiamo per questo il seguente teorema:
\begin{theorem}{Teorema di convergenza locale}
	Se si ha un intervallo $I \subseteq \mathbb{R}$, con $\alpha \in I$, $\phi(\alpha) = \alpha$, $\phi \in C^1(I)$ e esistono $\rho \in \mathbb{R}^+$ e $k \in (0, 1)$ tali che:
	$$
	|\phi'(x)| \leq k, \quad \forall x \in [\alpha - \rho, \alpha + \rho]
	$$
	Allora valgono:
	\begin{enumerate}
		\item $\forall x_0 \in [\alpha - \rho, \alpha + \rho] \implies x_n \in [\alpha - \rho, \alpha + \rho]$;
		\item $\forall x_0 \in [\alpha - \rho, \alpha + \rho]$ si ha:
			$$
				\lim_{n \rightarrow + \infty} x_n = \alpha
			$$
		\item $\alpha$ è l'unico punto fisso di $\phi(x)$ in $[\alpha - \rho, \alpha + \rho]$.
	\end{enumerate}
\end{theorem}

Vediamo di dimostrare.
\begin{enumerate}
	\item Il punto 1) si dimostra per induzione.
		Poste le ipotesi, il caso $n = 0$ è banale (si prende lo stesso punto).
		Vogliamo quindi porre l'errore, attraverso il teorema di Lagrange:
		$$
		|x_{n + 1} - \alpha| = |\phi(x_n) - \phi(\alpha)| = |x_n - \alpha| \cdot |\phi'(\varepsilon)|, \quad \varepsilon \in [x_n, \alpha]
		$$
		di questo abbiamo che il primo termine ($|x_n - \alpha|$) è $< \rho$, e il secondo ($|\phi'(\varepsilon)|$) è $<\leq k$, per cui l'errore successivo è:
		$$
		k \rho < \rho
		$$
		cioè:
		$$
		x_{n + 1} \in [\alpha - \rho, \alpha + \rho]
		$$ 
		che è la tesi. \qed

	\item Il punto 2) si dimostra a partire dallo stesso passaggio di prima:
		$$
		|x_{n + 1} - \alpha| = |x_n - \alpha| \cdot |\phi'(\varepsilon)| \leq k |x_n - \alpha|
		$$
		maggiorando $|\phi'(\varepsilon)|$ con $k$.
		Per il calcolo esplicito si ha quindi che all'$n$-esimo passaggio si ha:
		$$
		|x_n - \alpha| \leq k^{n + 1} \cdot \rho 
		$$
		e quindi basterà dire:
		$$
		\lim_{n \rightarrow +\infty} |x_{n + 1} - \alpha| \leq \lim_{n \rightarrow  +\infty} k^{n + 1} \cdot \rho = 0 
		$$
		che è  la tesi. \qed

	\item Infine, il punto 3) si dimostra supponendo per assurdo che $\exists \tilde{\alpha} \in [\alpha - \rho, \alpha + \rho]$, $\tilde{\alpha} \neq \alpha$ tale che $\phi(\tilde{\alpha}) = \tilde{\alpha}$.
		In questo caso varrà:
		$$
		|\alpha - \tilde{\alpha}| = | \phi(\alpha) - \phi(\tilde{\alpha}) | = | \alpha - \tilde{\alpha}| \cdot | \phi'(\varepsilon) | \leq |\alpha - \tilde{\alpha} | \cdot k < | \alpha - \tilde{\alpha} |
		$$
		che guardando agli estremi è un assurdo. \qed
\end{enumerate}

Osserviamo quindi che se $|\phi'(\alpha)| < 1$, il metodo converge localmente perché per continuità della derivata $\exists \rho, k$ tali che $\rho > 0$, $k \in (0, 1)$ e $|\phi'(x)| \leq k$ $\forall x \in [\alpha - \rho, \alpha + \rho]$.

Ulteriori osservazioni si possono fare sul tipo di convergenza: questa può essere \textbf{monotona} o \textbf{alternata}.
Infatti per l'errore vale:
$$
(x_{n + 1} - \alpha) = (x_n - \alpha) \cdot \phi'(\varepsilon), \quad \varepsilon \in [x_n, \alpha]
$$

\begin{itemize}
	\item 
Se $\phi'(x) > 0$ su $[\alpha - \rho, \alpha + \rho]$, allora gli errori $x_{n + 1} - \alpha$ e $x_n - \alpha$ hanno lo stesso segno, cioè la convergenza è monotona.
	\item
		Altrimenti, se $\phi'(x) < 0$ su $[\alpha - \rho, \alpha + \rho]$, allora gli errori $x_{n + 1} - \alpha$ e $x_n - \alpha$ hanno segno discorde, cioè la convergenza è alternata.
\end{itemize}

Possiamo poi dare il seguente teorema:
\begin{theorem}{Teorema sull'ordine di convergenza}
	Sia $\phi(x) \in C^p (I)$ e $\alpha$ punto fissso di $\phi$ (cioè $\alpha = \phi(\alpha)$) con $\alpha \in I$.
	Allora $\exists \rho > 0$ tale per cui $\forall x_0 \in [\alpha - \rho, \alpha + \rho]$ la successione $\{x_n\}$ converge con ordine $p \geq 1$ ad $\alpha$ se e solo se vale:
	$$
	\phi'(\alpha) = \phi''(\alpha) = ... = \phi^{(p - 1)}(\alpha) = 0, \quad \phi^{(p)}(\alpha) \neq 0
	$$
\end{theorem}

Osserviamo che $p$ deve essere chiaramente un numero intero (un indice di derivata).

Dimostriamo quindi le due coimplicazioni.
\begin{itemize}
	\item[$\Leftarrow$:] Abbiamo che $x_{n + 1} = \phi(x_n)$. Sviluppando con Taylor nel punto $\alpha$ si ha:
		$$
		x_{n + 1} = \phi(\alpha) + \phi'(\alpha)(x_n - \alpha) + ... + \phi^{(p - 1)}(\alpha) \frac{(x_n - \alpha)^{p - 1}}{(p - 1)!} + \phi^{(p)}(\varepsilon) \frac{(x_n - a)^p}{p!}
		$$
		$$
		= \phi(\alpha) + \phi^{(p)}(\varepsilon) \frac{(x_n - \alpha)^p}{p!}, \quad \varepsilon \in [\alpha, x_n]
		$$
		Quindi:
		$$
		x_{n + 1} - \alpha = \phi^{(p)} (\varepsilon) \frac{(x_n - \alpha)^p}{p!}
		\implies
		\frac{x_{n + 1} - \alpha}{(x_n - \alpha)^p} = \frac{\phi^{(p)}(\varepsilon)}{p!} 
		$$
		$$
		\implies
		\lim_{n \rightarrow + \infty} \frac{x_{n + 1} - \alpha}{(x_n - \alpha)^p} = \lim_{n \rightarrow +\infty} \frac{\phi^{(p)}(\varepsilon)}{p!} 
		$$
		Dato che $x_n \rightarrow \alpha$, si ha che:
		$$
		\lim_{n \rightarrow +\infty} \phi^{(p)} (\varepsilon) = \phi^{(p)} (\alpha)
		$$
		e:
		$$
		\lim_{n \rightarrow +\infty} \frac{|x_{n + 1} - \alpha|}{|x_n - \alpha|^p} = \frac{\phi^{(p)} (\alpha)}{p!} \neq 0
		$$

	\item[$\Rightarrow$:] Prendiamo $1 \leq r \leq p - 1$ e facciamo vedere che:
		$$
			\phi^{(r)} (\alpha) = 0
		$$
		per induzione su $r$.

		Se $r = 1$, si ha che:
		$$
	 	\lim_{n \rightarrow +\infty} \frac{|x_{n + 1} - \alpha|}{|x_n - \alpha|} = \lim_{n \rightarrow +\infty} \frac{|\phi(x_n) - \phi(\alpha)|}{|x_n - \alpha|} = \lim_{n \rightarrow + \infty} \frac{ |x_n - \alpha| \phi'(\varepsilon) }{|x_n - \alpha|} = \phi'(\alpha)
		$$

		Prendiamo quindi per vero che $0 = \phi'(\alpha) = \phi''(\alpha) = ... = \phi^{(r - 1)} (\varepsilon)$, e dimostriamo che vale anche per $r$.
		Per fare questo prenderemo lo sviluppo di Taylor in $\alpha$ troncato all'ordine $r$:
		$$
		\phi(x_n) - \phi(\alpha) = (x_n - \alpha) \phi'(\alpha) + \frac{(x_n - \alpha)^2}{2} \phi''(\alpha) + ... + \frac{(x_n - \alpha)^{r - 1}}{(r - 1)!} \phi^{(r - 1)} (\alpha) + \frac{(x_n - \alpha)^r}{r!} \phi^{(r)} (\varepsilon)
		$$
		$$
		= \frac{(x_n - \alpha)^r}{2!} \phi^{(r)}(\varepsilon), \quad \varepsilon \in [x_n, \alpha]
		$$
		da cui:
		$$
		\frac{x_{n + 1} - \alpha}{(x_n - \alpha)^r} = \frac{\phi^{(r)} (\varepsilon)}{r!} \implies \lim_{n \rightarrow +\infty} \frac{|x_{n + 1} - \alpha|}{|x_n - \alpha|^r} = \lim_{n \rightarrow +\infty} \frac{\phi^{(r)} (\varepsilon)}{r!} = \frac{\phi^{(r)}(\alpha)}{r!} \implies \phi^{(r)}(\alpha) = 0
		$$
		\qed
\end{itemize}

\end{document}
