
\documentclass[a4paper,11pt]{article}
\usepackage[a4paper, margin=8em]{geometry}

% usa i pacchetti per la scrittura in italiano
\usepackage[french,italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\frenchspacing 

% usa i pacchetti per la formattazione matematica
\usepackage{amsmath, amssymb, amsthm, amsfonts}

% usa altri pacchetti
\usepackage{gensymb}
\usepackage{hyperref}
\usepackage{standalone}

% imposta il titolo
\title{Appunti Calcolo Numerico}
\author{Luca Seggiani}
\date{2025}

% disegni
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}

% imposta lo stile
% usa helvetica
\usepackage[scaled]{helvet}
% usa palatino
\usepackage{palatino}
% usa un font monospazio guardabile
\usepackage{lmodern}

% tikz in sans
\tikzset{every picture/.style={/utils/exec={\sffamily}}}

\renewcommand{\rmdefault}{ppl}
\renewcommand{\sfdefault}{phv}
\renewcommand{\ttdefault}{lmtt}

% circuiti
\usepackage{circuitikz}
\usetikzlibrary{babel}

% disponi il titolo
\makeatletter
\renewcommand{\maketitle} {
	\begin{center} 
		\begin{minipage}[t]{.8\textwidth}
			\textsf{\huge\bfseries \@title} 
		\end{minipage}%
		\begin{minipage}[t]{.2\textwidth}
			\raggedleft \vspace{-1.65em}
			\textsf{\small \@author} \vfill
			\textsf{\small \@date}
		\end{minipage}
		\par
	\end{center}

	\thispagestyle{empty}
	\pagestyle{fancy}
}
\makeatother

% disponi teoremi
\usepackage{tcolorbox}
\newtcolorbox[auto counter, number within=section]{theorem}[2][]{%
	colback=blue!10, 
	colframe=blue!40!black, 
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries, 
	title=Teorema~\thetcbcounter: #2, 
	#1
}

% disponi definizioni
\newtcolorbox[auto counter, number within=section]{definition}[2][]{%
	colback=red!10,
	colframe=red!40!black,
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries,
	title=Definizione~\thetcbcounter: #2,
	#1
}

% disponi problemi
\newtcolorbox[auto counter, number within=section]{problem}[2][]{%
	colback=green!10,
	colframe=green!40!black,
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries,
	title=Problema~\thetcbcounter: #2,
	#1
}

% disponi codice
\usepackage{listings}
\usepackage[table]{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{codestyle}{
		backgroundcolor=\color{black!5}, 
		commentstyle=\color{codegreen},
		keywordstyle=\bfseries\color{magenta},
		numberstyle=\sffamily\tiny\color{black!60},
		stringstyle=\color{green!50!black},
		basicstyle=\ttfamily\footnotesize,
		breakatwhitespace=false,         
		breaklines=true,                 
		captionpos=b,                    
		keepspaces=true,                 
		numbers=left,                    
		numbersep=5pt,                  
		showspaces=false,                
		showstringspaces=false,
		showtabs=false,                  
		tabsize=2
}

\lstdefinestyle{shellstyle}{
		backgroundcolor=\color{black!5}, 
		basicstyle=\ttfamily\footnotesize\color{black}, 
		commentstyle=\color{black}, 
		keywordstyle=\color{black},
		numberstyle=\color{black!5},
		stringstyle=\color{black}, 
		showspaces=false,
		showstringspaces=false, 
		showtabs=false, 
		tabsize=2, 
		numbers=none, 
		breaklines=true
}

\lstdefinelanguage{javascript}{
	keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
	keywordstyle=\color{blue}\bfseries,
	ndkeywords={class, export, boolean, throw, implements, import, this},
	ndkeywordstyle=\color{darkgray}\bfseries,
	identifierstyle=\color{black},
	sensitive=false,
	comment=[l]{//},
	morecomment=[s]{/*}{*/},
	commentstyle=\color{purple}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	morestring=[b]',
	morestring=[b]"
}

% disponi sezioni
\usepackage{titlesec}

\titleformat{\section}
	{\sffamily\Large\bfseries} 
	{\thesection}{1em}{} 
\titleformat{\subsection}
	{\sffamily\large\bfseries}   
	{\thesubsection}{1em}{} 
\titleformat{\subsubsection}
	{\sffamily\normalsize\bfseries} 
	{\thesubsubsection}{1em}{}

% disponi alberi
\usepackage{forest}

\forestset{
	rectstyle/.style={
		for tree={rectangle,draw,font=\large\sffamily}
	},
	roundstyle/.style={
		for tree={circle,draw,font=\large}
	}
}

% disponi algoritmi
\usepackage{algorithm}
\usepackage{algorithmic}
\makeatletter
\renewcommand{\ALG@name}{Algoritmo}
\makeatother

% disponi numeri di pagina
\usepackage{fancyhdr}
\fancyhf{} 
\fancyfoot[L]{\sffamily{\thepage}}

\makeatletter
\fancyhead[L]{\raisebox{1ex}[0pt][0pt]{\sffamily{\@title \ \@date}}} 
\fancyhead[R]{\raisebox{1ex}[0pt][0pt]{\sffamily{\@author}}}
\makeatother

\begin{document}

% sezione (data)
\section{Lezione del 28-03-25}

% stili pagina
\thispagestyle{empty}
\pagestyle{fancy}

% testo
Riprendiamo il discorso sull'errore inerente dei sistemi lineari.

\subsubsection{Condizionamento in $\mathbf{ \delta A}$}
Avevamo preso delle perturbazioni sulle matrici $A$ e $b$ (dovute a vari effetti reali, quali errori di arrotondamento, di misura, ecc...) nella forma:
$$
(A + \delta A) (x + \delta x) = (b + \delta b)
$$
e volevamo capire quanto può essere grande l'errore relativo $\frac{|\delta x|}{|x|}$, da:
$$
\frac{ \text{sol. perturbata} - \text{sol. reale} }{ \text{sol. reale} } = \frac{|x + \delta x - x|}{|x|} = \frac{|\delta x|}{|x|}
$$
Nel caso di $\delta A = 0$, abbiamo visto di poter maggiorare tale quantità come:
$$
\frac{|\delta x|}{|x|} \leq \mu(A) \cdot \frac{|\delta b|}{|b|}
$$
con $\mu(A) = |A| \cdot |A^{-1}|$ \textit{numero di condizionamento} (definizione 9.1).

Riguardo a $\mu(A)$, si ha che è $\geq 1$, cioè chiaramente non si può ridurre l'errore oltre la perfezione, e se $\mu(A) \approx 10^{k}$, $k$ è il numero di cifre significative che si \textit{perdono} nel risultato $x + \delta x$.

Possiamo quindi reintrodurre il termine $\delta A$ ed enunciare il seguente teorema:
\begin{theorem}{Condizionamento in $\mathbf{ \delta A }$}
	Se $|\delta A| \cdot |A^{-1}| < 1$ allora si ha:
	$$
	\frac{| \delta x|}{|x|} \leq \frac{\mu(A)}{1 - \mu(A) \cdot \frac{ |\delta A| }{|A|}} \cdot \left( \frac{|\delta A|}{|A|} + \frac{|\delta b|}{|b|} \right)
	$$
\end{theorem}
dove osserviamo che se $\delta A = 0$ si ottiene la stessa diseguaglianza che abbiamo dimostrato col teorema 9.1.

La dimostrazione del teorema non è immediata.
Ci arriviamo in 3 iterazioni successive, restringendo man mano le approssimazioni fatte. 
\begin{enumerate}
	\item 
Una prima stima si potrebbe avere calcolando direttamente:
$$
(A + \delta A) (x + \delta x) = (b + \delta b) \implies Ax + A \delta x + \delta A x + \delta A \delta x = b + \delta b
$$

$Ax = b$ dalle ipotesi, mentre il termine $\delta A \delta b$, il prodotto di due errori, è trascurabile, quindi:
$$
A \delta x + \delta A x \approx \delta b \implies \delta x \approx A^{-1} \left( \delta b - \delta A x \right)
$$
Passando alle norme si ha:
\begin{equation}
| \delta x | \leq |A^{-1}| \left(|\delta b| + |\delta A| |x| \right)
\end{equation}
da cui dividendo per $|x| \geq \frac{|A|}{|b|}$ e moltiplicando e dividendo il termine a destra per $|A|$, si ha:
$$
\frac{|\delta x|}{|x|} \leq |A^{-1}| \left( \frac{|\delta b|}{|b|} |A| + |\delta A| \frac{|A|}{|A|} \right) = |A^{-1}| |A| \left( \frac{|\delta b|}{|b|} + \frac{|\delta A|}{|A|} \right) = \mu(A) \left( \frac{|\delta b|}{|b|} + \frac{|\delta A|}{|A|} \right)
$$
Da cui otteniamo essenzialmente la forma del teorema. 

\item
Per capire il denominatore del numero di condizionamento, che migliora il maggiorante nei casi dove $A$ è quasi singolare, adottiamo un procedimento diverso che evidenzia l'errore fatto sull'inversa $(A + \delta A)^{-1}$ (nel calcolo precedente, starebbe nel termine $\delta A \delta x$ che abbiamo trascurato).
Avremo quindi:
$$
(A + \delta A) (x + \delta x) = (b + \delta b) \implies (x + \delta x) = (A + \delta A)^{-1} (b + \delta b)
$$
$(A + \delta A)^{-1}$ risulta di difficile approssimazione.
Riscriviamolo come:
$$
(A + \delta A)^{-1} = (I + A^{-1} \delta A)^{-1} A^{-1}
$$
e consideriamo la serie di Von Neumann:
$$
(I - M)^{-1} = I + M + M^2 + ...
$$
troncando al primo termine, si ha che possiamo riscrivere la prima inversa come:
$$
(I + A^{-1} \delta A)^{-1} \approx I - A^{-1} \delta A 
$$
e quindi:
\begin{equation}
(A + \delta A)^{-1} \approx (I - A^{-1} \delta A ) A^{-1} = A^{-1} - A^{-1} \delta A A^{-1} 
\end{equation}
Possiamo convincerci che l'approssimazione è valida sostituendo nella formula per $x + \delta x$ e trovando:
$$
x + \delta x = (A^{-1} - A^{-1} \delta A A^{-1}) (b + \delta b) = A^{-1} b + A^{-1} \delta b - A^{-1} \delta A A^{-1} b - A^{-1} \delta A A^{-1} \delta b
$$
$x = A^{-1}$ dalle ipotesi, mentre il termine $A^{-1} \delta A A^{-1} \delta b$ contiene il prodotto di due errori, ed è trascurabile, quindi:
$$
\delta x \approx A^{-1} \delta b - A^{-1} \delta A A^{-1} b = A^{-1} \delta b - A^{-1} \delta A x = A^{-1} \left( \delta b - \delta A x \right)
$$
che è esattamente la forma della (1) che avevamo al primo metodo.

\item 
Riprendiamo la forma in (2) dell'inversa:
$$
(A + \delta A)^{-1} \approx (I - A^{-1} \delta A ) A^{-1} 
$$
per introdurre il denominatore che vediamo nell'enunciato del teorema.
Passando alle norme, si avrà:
$$
|I - A^{-1} \delta A| |A^{-1}| = \frac{|A^{-1}|}{1 + |A^{-1} \delta A|}
$$
Prendendo questa come la norma dell'inversa per il passaggio alle norme della (1), si ha:
$$
\frac{|\delta x|}{|x|} \leq \frac{|A^{-1}|}{1 + |A^{-1} \delta A|} \left( \frac{|\delta b|}{|b|} |A| + |\delta A| \frac{|A|}{|A|} \right) \leq \frac{ |A^{-1}| |A| }{ 1 + |A^{-1}| |\delta A| } \left( \frac{|\delta b|}{|b|} + \frac{|\delta A|}{|A|} \right)
$$
dove l'$|A^{-1}| |\delta A|$ al denominatore si può scrivere come $\mu(A) \frac{|\delta A|}{|A|}$, in quanto:
$$
|A^{-1}| |\delta A| = \mu(A) \cdot \frac{|\delta A|}{|A|} = |A||A^{-1|} \cdot \frac{|\delta A|}{|A|}
$$
da cui la forma del teorema. \qed
\end{enumerate}

\subsubsection{Stima di mu}
In genere è abbastanza costoso calcolare il numero di condizionamento, in quanto bisogna calcolare un inversa e quindi la sua norma.
Quello che si può fare è cercarne una stima.

Ad esempio, se $A$ è hermitiana ($A = A^H$) e si considera la norma euclidea $|\cdot|_2$, si ha che:
$$
|A|_2 \cdot |A^{-1}|_2 = \rho(A) \cdot \rho(A^{-1}) = \frac{\lambda_\text{max}(A)}{\lambda_\text{min}(A)}
$$
cioè prendiamo il rapporto fra l'autovalore più grande e l'autovalore più piccolo di $A$, per cui si possono usare i metodi per gli autovalori (che vedremo verso la fine del corso).

\subsubsection{Stime a posteriori}
Supponiamo di aver calcolato $\tilde{x} \in \mathbb{C}^n$ con un qualunque metodo di approssimazione di $x$, prese $A$ e $b$ per buone.
Per valutare se $\tilde{x}$ è una \textit{buona} approssimazione basta guardare al \textbf{vettore residuo}, cioè:
$$
r = b - A \tilde{x}
$$

Potremmo chiederci se, con $|r|$ piccolo, si hanno anche $|x - \tilde{x}|$ piccoli.
Sottraiamo allora $Ax = b$ da $r$:
$$
A \left( x - \tilde{x} \right) = r \implies x - \tilde{x} = A^{-1} r
$$
e quindi vale la diseguaglianza:
$$
|x - \tilde{x} | \leq |A^{-1}| |r|
$$

Usando:
$$
|x| \geq \frac{|b|}{|A|}
$$
si otterrà allora che:
$$
\frac{|x - \tilde{x}|}{|x|} \leq \frac{ |A| |A^{-1}| |r| }{ |b| } = \mu(A) \cdot \frac{ |r| }{ |b| }
$$

Allora, in problemi ben condizionati, avremo che $\frac{ |x - \tilde{x}| }{|x|}$ errore relativo e $|r|$ sono comparabili, mentre in problemi con condizionamento $\mu(A) >> 1$ si potrebbe avere:
$$
\frac{ \frac{ |x - \tilde{x}| }{|x|} }{ |r| } \approx \mu(A)
$$

\subsubsection{Tecniche per l'approssimazione delle inverse}
Come nota conclusiva della sezione sul'errore nei sistemi lineari, notiamo che ogni volta che c'è bisogno di risolvere una forma del tipo:
$$
Ax = b \implies x = A^{-1} b
$$
il metodo naive sarebbe quello di calcolare $A^{-1}$ e moltiplicare per $b$.
Vediamo se si può fare di meglio.

Preso $n = 1$, la forma sarà quella di una semplice equazione lineare:
$$
ax = b. \implies x = \frac{b}{a}
$$
cioè basta fare una sola divisione, mentre l'approccio naive risulterebbe nel:
\begin{enumerate}
	\item Calcolare il reciproco $a^{-1}$;
	\item Moltiplicare il reciproco per $b \implies x = a^{-1} \cdot b$. 
\end{enumerate}
che chiaramente risulta in più passaggi, e quindi più approssimazioni intermedie e in definitiva maggiore errore.

\lstset{language=MATLAB, style=codestyle}

Nel caso matriciale il metodo di divisione equivale a fare una divisione matrice-vettore (che in MATLAB si effettua come \lstinline|A \ b|), di complessità $O( \frac{2}{3} n^3 )$.
Di contro, il metodo naive (che in MATLAB si effettua come \lstinline|inv(A) * b|) avrà complessità intorno ad $O( \frac{8}{3} n^3 )$, in quanto calcola la fattorizzazione LU e risolve 2n sistemi triangolari.

Inoltre, l'approccio naive è anche meno accurato, in quanto se il numero di condizionamento $\mu(A)$ è alto, l'errore di approssimazione nei passaggi intermedi potrebbe accumularsi molto di più che rispetto all'approccio della semplice divisione matrice-vettore (tanto che la stessa documentazione di MATLAB suggerisce di evitarlo).

\subsection{Metodi iterativi per i sistemi lineari}
Veniamo quindi a trattare i metodi iterativi per la risoluzione dei sistemi lineari.

L'idea è quella di approssimare la soluzione di un sistema lineare $Ax = b$ generando una successione di vettori $\{ x^{(k)} \}_{k \in \mathbb{N}}$ tali che:
$$
\lim_{k \rightarrow +\infty} x^{(k)} = x
$$
La motivazione è chiaramente quella di eludere l'alta complessità di $\sim O(n^3)$ che ha la risoluzione con metodi diretti.
Altra motivazione potrebbe essere quella di non conoscere direttamente $A$, ma solo l'applicazione:
$$
v \rightarrow A \cdot v
$$
(si pensi, anche se rappresenta un caso \textit{non} lineare, ai metodi di discesa a gradiente che ottimizzano funzioni non immediatamente calcolabili o anche solo esprimibili).

Abbiamo quindi che un buon metodo iterativo dovrà:
\begin{enumerate}
	\item Costare meno di $O(n^3)$ ad ogni passaggio (altrimenti sarebbe inutile rispetto ai metodi diretti), e quindi richiedere solo prodotti di matrici con vettori, o risoluzione di sistemi lineari favorevoli (triangolari, diagonali, ecc...);
	
	\item Data una certa \textbf{accuratezza} posta come obiettivo, impiegare un numero ragionevole di iterazioni per raggiungerla.
\end{enumerate}

\subsubsection{Metodi di punto fisso}
L'idea è quella di partire da $Ax - b = 0$ e di riscrivere come un'equazione equivalente in forma:
$$
x = Hx + c, \quad H \in \mathbb{C}^{n \times m}, \ c \in \mathbb{C}^n
$$

Facciamo alcuni esempi:
\begin{enumerate}
	\item Si sceglie una matrice $G \in \mathbb{C}^{n \times n}$ invertibile e si considera:
		$$
		x = x \cdot G(Ax - b) = (I - GA) x + Gb
		$$
		cioè:
		$$
		H = I - GA, \quad c = Gb
		$$

	\item Si scompone $A$ come:
		$$
		A = A_1 + A_2
		$$
		per cui:
		$$
		Ax = b \ \Leftrightarrow \ (A_1 + A_2) c = b \ \Leftrightarrow \ A_1 x = - A_2 x + b \ \Leftrightarrow \ x = -A_1^{-1} A_2 x + A_1^{-1} b
		$$
		cioè ancora:
		$$
		H = -A_1^{-1}A_2 , \quad c = A_1^{-1} b
		$$
\end{enumerate}

Una volta trovata un'equazione di punto fisso $x = H x + c$, quindi, si considera il seguente metodo iterativo:
\[
	\begin{cases}
		x^{(0)} \text{ dato} \\
		x^{(k + 1)} = H x^{(k)} + c, \quad k = 1, 2, 3, ...
	\end{cases}
\]
partendo da $x^{(0)}$ come dato a priori (sarà la soluzione che vorremo raffinare).

Vediamo che infatti:
\begin{definition}{Matrice di iterazione}
	La matrice $H$ di un equazione di punto fisso $x = H x + c$ viene detta matrice di iterazione.
\end{definition}

Per avere una verifica della validità dei metodi di punto fisso, enunciamo il seguente teorema:
\begin{theorem}{Validità dei metodi di punto fisso}
	Il metodo di punto fisso dato da:
\[
	\begin{cases}
		x^{(0)} \text{ dato} \\
		x^{(n + 1)} = H x^{(k)} + c, \quad k = 1, 2, 3, ...
	\end{cases}
\]
converge $\forall x^{(0)} \in \mathbb{C}^n$ se e solo se $\rho(H) < 1$.
\end{theorem}

Questo si dimostra prendendo l'equazione di punto fisso $x = H x + c$, soddisfatta da $x$ soluzione esatta, per cui:
$$
x^{(k + 1)} - x = H x^{(k)} + c - (H x + c) = H ( x^{(k)} - x )
$$

Possiamo chiamare $e^{(k)} = x^{(k)} - x$, cioè l'errore al passo $k$, e quindi l'errore al passo $k + 1$ sarà:
$$
e^{(k + 1)} = H e^{(k)} = H^k e^{(0)}
$$

Basterà allora prendere il limite:
$$
\lim_{k \rightarrow + \infty} e^{(k + 1)} = \lim_{k \rightarrow + \infty} H^k e^{(0)}
$$

Perchè questo tenda a $0$, basterà imporre $\rho(H) < 1$, in quanto in tal caso:
$$
\lim_{k \rightarrow + \infty} H^k e^{(0)} = 0
$$
qualsiasi sia l'errore iniziale $e^{(0)}$ (e quindi la scelta di soluzione iniziale $x^{(0)})$. \qed

In particolare, possiamo ricordare che se $|H| < 1 \implies \rho(H) < 1$, quindi può risultare verificare questa condizione, che è sì più stretta ma anche più facile da verificare.
Ricordiamo che non vale assolutamente il contrario, cioè $|H| > 1 \;\not\!\!\!\implies \rho(H) > 1$. 

Di contro, vale anche la condizione $|\det(H)| \geq 1 \implies \rho(H) > 1$, che come prima non si inverte in $|\det(H)| < 1 \;\not\!\!\!\implies \rho(H) < 1$.

\subsubsection{Velocità di convergenza}
Guardando alla dimostrazione del teorema 10.2, possiamo osservare che il raggio spettrale $\rho(H)$ ci dà un informazione anche riguardo alla \textbf{velocità di convergenza} del metodo di punto fisso.

Infatti, si avrà che vale:
$$
\frac{|e^{(k)}|}{|e^{(0)}|} \leq | H^k |
$$

Dall'algebra lineare, si ha che quando $k$ è abbastanza grande, $H^k \approx \rho(H)^k$, almeno per norme indotte, in quanto:
$$
\lim_{k \rightarrow +\infty} \sqrt[k]{|H^k|} = \rho(H)
$$

Questo ci dice che se si hanno 2 metodi di punto fisso con matrici di iterazione $H_1$ e $H_2$ tali che:
$$
\rho(H_1) < \rho(H_2) < 1
$$
allora il primo metodo converge più velocemente del secondo, è dall'ulteriore ipotesi $<1$, entrambi convergono.

Inoltre, si può stimare il numero di iterazioni $k$ necessarie a raggiungere un certo valore dell'errore:
$$
\frac{ | e^{(k)} | }{ | e^{(0)} | } = \frac{ | x^{(k)} - x | }{ | e^{(0)} | } \leq \delta 
$$
infatti, in tal caso basterà imporre:
$$
\rho(H)^k \leq \delta \implies k \geq \frac{ \log(\delta) }{\log( \rho(H) )}
$$

\subsubsection{Criteri di stop}
Molto spesso non è pratico decidere un errore e fare le $k$ iterazioni che dovrebbero portare a tale errore (in quanto non è scontato conoscere $\rho(H)$), ma bensì si preferisce definire un \textbf{criterio di stop} per la terminazione dell'algoritmo raggiunte determinate condizioni.

Queste condizioni possono essere:
\begin{enumerate}
	\item Si può restringere il residuo:
		$$
			\frac{ | r^{(k)} | }{ | b | } < \delta	
		$$
	\item Si può restringere direttamente la variazione di errore:
		$$
			| x^{(k + 1)} - x^{(k)} | < \delta	
		$$
\end{enumerate}

\subsection{Metodi di Jacobi e Gauss-Seidel}
Vediamo i metodi più famosi di questo tipo, detti metodi di \textbf{Jacobi} e \textbf{Gauss-Seidel}.

L'idea è di scomporre la matrice $A$ come:
$$
A = D - E - F =
$$
$$
\begin{pmatrix}
	d_1 & 0 & ... & 0 \\
	0 & d_2 & ... & 0 \\
	0 & ... & d_{n-1} & 0 \\
	0 & ... & 0 & d_n \\
\end{pmatrix}
-
\begin{pmatrix}
	0 & 0 & ... & 0 \\
	-e_{21} & 0 & ... & 0 \\
	-e_{n-1,1} & ... & 0 & 0 \\
	-e_{n1} & ... & -e_{n, n-1} & 0 \\
\end{pmatrix}
-
\begin{pmatrix}
	0 & -f_{12} & ... & -f_{1n} \\
	0 & 0 & ... & -f_{2n} \\
	0 & ... & 0 & -f_{n-1, n} \\
	0 & ... & 0 & 0 \\
\end{pmatrix}
$$
con $D$ diagonale, $E$ l'opposta della triangolare inferiore a diagonale nulla e $F$ l'opposta della triangolare superiore a diagonale nulla.
Varrà quindi:
$$
Ax = b \ \Leftrightarrow \ (D - E - F) x = b
$$

\subsubsection{Metodo di Jacobi}
Il metodo di Jacobi consiste nel riscrivere quanto trovato come:
$$
D x = (E + F) x + b \implies x = D^{-1} (E + F) x + D^{-1} b
$$
cioè trovare un equazione di punto fisso con:
$$
H = D^{-1} (E + F), \quad c = D^{-1} b
$$
e quindi applicare l'algoritmo:\[
	\begin{cases}
		x^{(0)} \text{ dato} \\
		x^{(k + 1)} = D^{-1} (E + F) x^{(k)} + D^{-1} b = D^{-1} ( (E + F) x^{(k)} + b )
	\end{cases}
\]

Osserviamo che ad ogni iterazione si calcola un prodotto matrice-vettore e si risolve un sistema diagonale ($O(n)$), in quanto la $D$ si inverte facilmente (è diagonale):
$$
H_J = D^{-1} (E + F) = 
\begin{pmatrix}
	\frac{1}{d_1} & 0 & ... & 0 \\
	0 & \frac{1}{d_2} & ... & 0 \\
	0 & ... & \frac{1}{d_{n-1}} & 0 \\
	0 & ... & 0 & \frac{1}{d_n} \\
\end{pmatrix}
\begin{pmatrix}
	0 & -f_{12} & ... & -f_{1n} \\
	-e_{21} & 0 & ... & -f_{2n} \\
	-e_{n-1,1} & ... & 0 & -f_{n-1, n} \\
	-e_{n1} & ... & -e_{n, n-1} & 0 \\
\end{pmatrix}
$$
per cui:
$$
(H_J)_{ij} = 
\begin{cases}
	-\frac{ a_{ij} }{a_{ii}}, \quad i \neq j \\ 
	0 \quad i = j
\end{cases}
$$

Le matrici che descriveranno il passo di Jacobi saranno allora:
$$
x_i^{(k + 1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{i \neq j}^n a_{ij} x_j^{(k)}  \right)
$$
osservando che per calcolare $x_i^{(k + 1)}$ serviranno \textit{tutte} le componenti di $x^{(k)}$, cioè in codice bisogna mantenere due vettori, in quanto non si può sovrascrivere $x^{(k)}$ prima di aver finito di calcolare $x^{(k + 1)}$.

\subsubsection{Implementazione MATLAB del metodo di Jacobi}
In MATLAB, una possibile implementazione che sfrutta le funzionalità di calcolo matriciale può essere la seguente:

\lstset{style=codestyle, language=MATLAB}
\lstinputlisting{../code/matlab/jacobi.m}

\subsubsection{Implementazione C++ del metodo di Jacobi}
Possiamo sfruttare la forma scalare del metodo di Jacobi per realizzarne un'implementazione direttamente in C++.
Definiamo inizialmente alcune funzioni helper in un header, \lstinline|mat.h|, che useremo anche nell'esempio successivo a questo:
\lstset{style=codestyle, language=c++}
\lstinputlisting{../code/cpp/mat.h}

E definiamo quindi l'algoritmo come segue:
\begin{lstlisting}[language=C++, style=codestyle]	
double* jacobi(unsigned n, unsigned k, double** A, double* b) {
	// inizializza vettori
	double* v1, *v2;
	v1 = new double[n];
	v2 = new double[n];

	for(int r = 0; r < n; r++) {
		v1[r] = 0;
	}

	for(int i = 0; i < k; i++) {
		std::cout << "Iterazione " << i + 1 << std::endl;

		std::cout << "Il vettore vecchio e':" << std::endl;
		print_vector(v1, n);
		
		for(int r = 0; r < n; r++) {
			v2[r] = b[r];

			for(int c = 0; c < n; c++) {
				if(r == c) continue;

				v2[r] -= A[r][c] * v1[c];
			}

			v2[r] /= A[r][r];
		}

		vec_copy(n, v2, v1);

		std::cout << "Il vettore nuovo e':" << std::endl;
		print_vector(v1, n);
		std::cout << std::endl;
	}

	// ripulisci
	delete v2;
	
	return v1;
}
\end{lstlisting}

A questo punto un programma dovrà semplicemente usare le funzioni per il caricamento dell'ambiente di \lstinline|mat.h| e chiamare la funzione \lstinline|jacobi()|:
\begin{lstlisting}[language=C++, style=codestyle]	
int main() {
	// inizializza ambiente
	unsigned n, k; double** A; double* b;
	init_env(n, k, A, b);

	double* x = jacobi(n, k, A, b);

	std::cout << "Il risultato finale e':" << std::endl;
	print_vector(x, n);
	std::cout << std::endl;

	delete[] x;

	// ripulisci
	clean_env(n, A, b);
	return 0;
}
\end{lstlisting}

\subsubsection{Metodo di Gauss-Seidel}
Il metodo di Gauss-Seidel consiste nel riscrivere quanto trovato come:
$$
(D - E) x = F x + b \implies x = (D - E)^{-1} F x + (D - E)^{-1} b
$$cioè trovare un equazione di punto fisso con:
$$
H = (D - E)^{-1} F, \quad c = (D - E)^{-1} b 
$$
e quindi applicare l'algoritmo:\[
	\begin{cases}
		x^{(0)} \text{ dato} \\
		x^{(k + 1)} = (D - E)^{-1} F x^{(k)} + (D - E)^{-1} b
	\end{cases}
\]

Avremo che la matrice di iterazione è:
$$
H_{GS} = (D - E)^{-1} F
$$
di cui osserviamo che la prima colonna è il vettore di zeri in quanto $F$ ha anch'essa la prima colonna al vettore di zeri.

In ogni caso, notiamo che non è necessario esplicitare $H_{GS}$, ma ad ogni iterazione basta calcolare il prodotto matrice-vettore:
$$
x^{(k)} \rightarrow F x^{(k)}
$$
e risolvere col metodo di sostituzione all'indietro il sistema lineare: 
$$
(D - E) y = F x^{(k)}
$$
Stesso discorso per $c$, che si trova sempre risolvendo una sola volta, col metodo di sostituzione all'indietro, il sistema lineare:
$$
(D - E) c = b
$$

\subsubsection{Implementazione MATLAB del metodo di Gauss-Seidel}
In MATLAB, una possibile implementazione che sfrutta le funzionalità di calcolo matriciale (in particolare la funzione per la sostituzione all'indietro \lstinline|bck_subst()| che abbiamo già implementato) può essere la seguente:

\lstset{style=codestyle, language=MATLAB}
\lstinputlisting{../code/matlab/seidel.m}

\subsubsection{Ottimizzazione del metodo di Gauss-Seidel}
L'implementazione vista adesso del metodo di Gauss-Seidel non è la più efficiente. 
Vediamo infatti che, preso:
$$
x^{(k + 1)} = (D - E)^{-1} F x^{(k)} + (D - E)^{-1} b
$$
e moltiplicando per $D^{-1}(D - E)$ si ha:
$$
x^{(k + 1)} = D^{-1} E x^{(k + 1)} + D^{-1} F x^{(k)} + D^{-1} b
$$

Esplicitare una dipendenza di $x^{(k + 1)}$ da sé stessa potrebbe sembrare poco intuitivo,
ma è invece conveniente in quanto ogni elemento di $x^{(k + 1)}$ dipenderà dai soli elementi precedenti, cioè si avrà:
$$
x_i^{(k + 1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j = 1}^{i - 1} a_{ij} x_j^{(k + 1)} - \sum_{j = i + 1}^n a_{ij} x_j^{(k)} \right), \quad i = 1, ..., n
$$

In questo modo si hanno due vantaggi:
\begin{enumerate}
	\item Non occorre risolvere sistemi triangolari;
	\item Per calcolare $x_i^{(k + 1)}$ non si ha bisogno di $x_h^{k}$ per $h < i$, e quindi si possono sovrascrivere le entrate di $x^{(k)}$, e mantenere un unico vettore.
\end{enumerate}

\subsubsection{Implementazione C++ del metodo di Gauss-Seidel}
Vediamo come si può usare quest'ultima versione (scalare) del metodo di Gauss-Seidel per realizzarne un'implementazione C++, sfruttando la stessa libreria \lstinline|mat.h| dell'esempio 10.2.3:
\begin{lstlisting}[language=C++, style=codestyle]	
double* jacobi(unsigned n, unsigned k, double** A, double* b) {
	// inizializza vettori
	double* v1;
	v1 = new double[n];

	for(int r = 0; r < n; r++) {
		v1[r] = 0;
	}

	for(int i = 0; i < k; i++) {
		std::cout << "Iterazione " << i + 1 << std::endl;

		std::cout << "Il vettore vecchio e':" << std::endl;
		print_vector(v1, n);
		
		for(int r = 0; r < n; r++) {
			v1[r] = b[r];

			for(int c = 0; c < n; c++) {
				if(r == c) continue;

				v1[r] -= A[r][c] * v1[c];
			}

			v1[r] /= A[r][r];
		}

		std::cout << "Il vettore nuovo e':" << std::endl;
		print_vector(v1, n);
		std::cout << std::endl;
	}
	
	return v1;
}
\end{lstlisting}

Come vediamo, la proprietà dimostrata comporta modifiche minime al codice (la semplice rimozione del vettore \lstinline|v2|, con prodotti scalari che vengono fatti tutti su \lstinline|v1|).

La realizzazione di un programma che esegue questo algoritmo è ancora analoga all'esempio 10.2.3 (implementazioni complete si trovano in \lstinline|../matlab/code|).

\subsubsection{Jacobi/Gauss-Seidel e matrici di permutazione}
Un'ultima osservazione è che sia Jacobi che Gauss-Seidel hanno come condizione che $a_{ii} \neq 0, \, \forall i$, in quanto altrimenti $D$ diagonale non sarebbe invertibile.
Se questa condizione non è soddisfatta, si possono fare delle trasformazioni "indolori" sul sistema per ritrovare la diagonale non nulla (applicare matrici di permutazione e applicare il metodo sul sistema permutato, per trovare poi una soluzione che al limite andrà \textit{de}-permutata).

Ad esempio, si può pensare di applicare Jacobi a:
$$
\Pi A x = \Pi b
$$
per trovare una qualche permutazione $\Pi x$ di $x$.

\end{document}
