
\documentclass[a4paper,11pt]{article}
\usepackage[a4paper, margin=8em]{geometry}

% usa i pacchetti per la scrittura in italiano
\usepackage[french,italian]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\frenchspacing 

% usa i pacchetti per la formattazione matematica
\usepackage{amsmath, amssymb, amsthm, amsfonts}

% usa altri pacchetti
\usepackage{gensymb}
\usepackage{hyperref}
\usepackage{standalone}

% imposta il titolo
\title{Appunti Calcolo Numerico}
\author{Luca Seggiani}
\date{2025}

% disegni
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.9}

% imposta lo stile
% usa helvetica
\usepackage[scaled]{helvet}
% usa palatino
\usepackage{palatino}
% usa un font monospazio guardabile
\usepackage{lmodern}

\renewcommand{\rmdefault}{ppl}
\renewcommand{\sfdefault}{phv}
\renewcommand{\ttdefault}{lmtt}

% disponi il titolo
\makeatletter
\renewcommand{\maketitle} {
	\begin{center} 
		\begin{minipage}[t]{.8\textwidth}
			\textsf{\huge\bfseries \@title} 
		\end{minipage}%
		\begin{minipage}[t]{.2\textwidth}
			\raggedleft \vspace{-1.65em}
			\textsf{\small \@author} \vfill
			\textsf{\small \@date}
		\end{minipage}
		\par
	\end{center}

	\thispagestyle{empty}
	\pagestyle{fancy}
}
\makeatother

% disponi teoremi
\usepackage{tcolorbox}
\newtcolorbox[auto counter, number within=section]{theorem}[2][]{%
	colback=blue!10, 
	colframe=blue!40!black, 
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries, 
	title=Teorema~\thetcbcounter: #2, 
	#1
}

% disponi definizioni
\newtcolorbox[auto counter, number within=section]{definition}[2][]{%
	colback=red!10,
	colframe=red!40!black,
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries,
	title=Definizione~\thetcbcounter: #2,
	#1
}

% disponi problemi
\newtcolorbox[auto counter, number within=section]{problem}[2][]{%
	colback=green!10,
	colframe=green!40!black,
	sharp corners=northwest,
	fonttitle=\sffamily\bfseries,
	title=Problema~\thetcbcounter: #2,
	#1
}

% disponi codice
\usepackage{listings}
\usepackage[table]{xcolor}

\lstdefinestyle{codestyle}{
		backgroundcolor=\color{black!5}, 
		commentstyle=\color{codegreen},
		keywordstyle=\bfseries\color{magenta},
		numberstyle=\sffamily\tiny\color{black!60},
		stringstyle=\color{green!50!black},
		basicstyle=\ttfamily\footnotesize,
		breakatwhitespace=false,         
		breaklines=true,                 
		captionpos=b,                    
		keepspaces=true,                 
		numbers=left,                    
		numbersep=5pt,                  
		showspaces=false,                
		showstringspaces=false,
		showtabs=false,                  
		tabsize=2
}

\lstdefinestyle{shellstyle}{
		backgroundcolor=\color{black!5}, 
		basicstyle=\ttfamily\footnotesize\color{black}, 
		commentstyle=\color{black}, 
		keywordstyle=\color{black},
		numberstyle=\color{black!5},
		stringstyle=\color{black}, 
		showspaces=false,
		showstringspaces=false, 
		showtabs=false, 
		tabsize=2, 
		numbers=none, 
		breaklines=true
}

\lstdefinelanguage{javascript}{
	keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
	keywordstyle=\color{blue}\bfseries,
	ndkeywords={class, export, boolean, throw, implements, import, this},
	ndkeywordstyle=\color{darkgray}\bfseries,
	identifierstyle=\color{black},
	sensitive=false,
	comment=[l]{//},
	morecomment=[s]{/*}{*/},
	commentstyle=\color{purple}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	morestring=[b]',
	morestring=[b]"
}

% disponi sezioni
\usepackage{titlesec}

\titleformat{\section}
	{\sffamily\Large\bfseries} 
	{\thesection}{1em}{} 
\titleformat{\subsection}
	{\sffamily\large\bfseries}   
	{\thesubsection}{1em}{} 
\titleformat{\subsubsection}
	{\sffamily\normalsize\bfseries} 
	{\thesubsubsection}{1em}{}

% disponi alberi
\usepackage{forest}

\forestset{
	rectstyle/.style={
		for tree={rectangle,draw,font=\large\sffamily}
	},
	roundstyle/.style={
		for tree={circle,draw,font=\large}
	}
}

% disponi algoritmi
\usepackage{algorithm}
\usepackage{algorithmic}
\makeatletter
\renewcommand{\ALG@name}{Algoritmo}
\makeatother

% disponi numeri di pagina
\usepackage{fancyhdr}
\fancyhf{} 
\fancyfoot[L]{\sffamily{\thepage}}

\makeatletter
\fancyhead[L]{\raisebox{1ex}[0pt][0pt]{\sffamily{\@title \ \@date}}} 
\fancyhead[R]{\raisebox{1ex}[0pt][0pt]{\sffamily{\@author}}}
\makeatother

\begin{document}

% sezione (data)
\section{Lezione del 28-02-25}

% stili pagina
\thispagestyle{empty}
\pagestyle{fancy}

% testo
\subsection{Operazioni sui numeri macchina}
Abbiamo introdotto l'insieme dei numeri macchina.
Vediamo adesso come eseguire \textbf{operazioni} fra elementi di questi insiemi.

Notiamo che, di base, dati $x, y \in F(\beta, m, L, U)$, non necessariamente $x \circ y \in F(\beta, m, L, U)$ per le comuni operazioni aritmetiche $+, -, \times, \div$.

Quello che faremo è quindi approssimare tali operazioni, cioè dire:
\begin{itemize}
	\item $x \oplus y = Rn(x + y)$
	\item $x \ominus y = Rn(x - y)$
	\item $x \otimes y = Rn(x \times y)$
	\item $x \oslash y = Rn(x \div y)$
\end{itemize}

Effetto di questa approsimazione è negare proprietà note dei reali, come ad esempio l'associativa:
$$
(x \oplus y) \oplus z \neq x \oplus (y \oplus z)
$$
$$
(x \oplus y) \otimes z \neq x \oplus (y \otimes z)
$$
Cioè, la valutazione di una formula con ordini diversi ma equivalenti in aritmetica esatta può portare a risultati differenti nell'insieme dei numeri di mmacchina.

\subsubsection{Errore nel calcolo di funzione}
Sia $f:\mathbb{R}^m \rightarrow \mathbb{R}$, e si voglia calcolare $f(P_0)$, con $P_0 =\left(x_1^{(0)}, x_2^{(0)}, ...x_m^{(0)}\right) \in \mathbb{R}^m$.
Le operazioni aritmetiche $+, -, \times, \div$ possono essere viste come funzioni di questo tipo.
Ci interroghiamo quindi sulla fonte dell'errore nella loro valutazione:
\begin{enumerate}
	\item Nel caso contenga funzioni irrazionali o trascendenti, $f$ verrà approssimata con una funzione $\overline{f}$ che coinvolge solo operazioni aritmetiche di base $+, -, \times, \div$;
	\item $\overline{f}$ viene tradotta in un \textit{algoritmo} $\overline{f}_a$, ovvero in una formula che coinvolge $\oplus, \ominus, \otimes, \oslash \leftarrow +, -, \times, \div$. La formula ottenuta finora viene detta \textbf{algoritmo};
	\item Potrebbe essere che $P_0 \neq F(\beta, m, L, U)$, e quindi viene approssimato a $P_1 = Rn(P_0)$.
\end{enumerate}

Quindi, vogliamo $f(P_0)$ ma possiamo solo approssimarla come $\overline{f}_a(P_1)$.

Ad esempio, poniamo di voler calcolare $e^\pi$.
I passaggi nell'ordine appena visto saranno:
\begin{enumerate}
	\item Approssimamo l'esponenziale al secondo grado dello svillupppo di Taylor:
		$$
			e^x \approx 1 + x + \frac{x^2}{2} = \overline{f}(x)
		$$
	\item Si riporta la $\overline{f}(x)$ a $\overline{f}_a(x)$:
		$$
			1 \oplus \left( x \oplus ( (x \otimes x) \oslash 2) \right)
		$$
		Indicheremo algoritmi di questo tipo anche attraverso \textbf{risultati intermedi}:
		\begin{itemize}
			\item $r_1 = x \cdot x$
			\item $r_2 = \frac{r_1}{2}$
			\item $r_3 = x + r_2$
			\item $r_4 = 1 + r_3$
		\end{itemize}
		con il risultato finale inteso come l'ultimo risultato intermedio.
		Un modo di visualizzare i risultati intermedi di un algoritmo può essere un albero:
		\begin{center}
			\begin{forest}
				[
					{$r_4 = 1 + r_3$}
					[
						{$r_3 = x + r_2$}
						[
							{$r_2 = \frac{r_1}{2}$}
							[
								{$r_1 = x \cdot x$}
								[$x$]
								[$x$]
							]
						]
					]
				]
			\end{forest}
		\end{center}
		dove la \textit{radice} rappresenta il \textbf{risultato} e le \textit{foglie} rappresentano le variabili della funzione.
	\item Si approssima $\pi$ al numero macchina più vicino:
		$$
			Rn(\pi) = 3.1415 = P_1
		$$
\end{enumerate}

Avremo quindi la formula finale:
$$
1 \oplus \left( P_1 \oplus ( (P_1 \otimes P_1) \oslash 2) \right)
$$

\par\medskip

Diamo quindi la definizione di \textbf{errore assoluto}:
\begin{definition}{Errore assoluto}
	Data $f : \mathbb{R}^m \rightarrow \mathbb{R}$, un punto $P_0 \in \mathbb{R}^m$ ed un algoritmo $\overline{f}_a$, l'errore assoluto è dato da:
	$$
		\sigma_f = \overline{f}_a (P_1) - f(P_0), \quad P_1 = Rn(P_0)
	$$
\end{definition}

e di errore relativo:
\begin{definition}{Errore relativo}
	Date le ipotesi della definizione 2.1, l'errore relativo è dato da:
	$$
	\epsilon_f = \frac{\overline{f}_a (P_1) - f(P_0)}{f(P_0)} = \frac{\sigma_f}{f(P_0)}
	$$
\end{definition}


\subsubsection{Errore di funzioni razionali}
Assumiamo per adesso $f$ \textbf{funzione razionale}, ovvero $f$ si può definire con un numero di operazioni in $+, -, \times, \div$.
Assumere funzioni razionali ci permette di prendere $f = \overline{f}$ e $f_a = \overline{f}_a$ (non ci sono irrazionali da riportare ai razionali).
Potremo allora dire:

$$
\sigma_f = f_a(P_1) - f(P_0) = f_a(P_1) - f(P_1) + f(P1) - f(P_0)
$$
che possiamo dividere in:
$$
\sigma_f = \sigma_a + \sigma_d, \quad \sigma_a = f_a(P_1) - f(P_1), \quad \sigma_d = f(P_1) - f(P_0)
$$
che chiamiamo rispettivamente \textbf{errore assoluto algoritmico} e \textbf{errore assoluto inerente}.

Allo stesso modo possiamo definire l'\textbf{errore relativo}:
$$
\epsilon_f = \frac{\sigma_f}{f(P_0)} = \frac{f_a(P_1) - f(P_0)}{f(P_0)} = \frac{f_a(P_1) - f(P_1)}{f(P_0)} + \frac{f(P_1) - f(P_0)}{f(P_0)}
$$
$$
= \frac{f_a(P_1) - f(P_1)}{f(P_1)} \cdot \frac{f(P_1)}{f(P_0)} + \frac{f(P_1) - f(P_0)}{f(P_0)} 
$$
che si divide nuovamente in :
$$
\epsilon_f = \epsilon_a + \epsilon_d, \quad \epsilon_a = \frac{f_a(P_1) - f(P_1)}{f(P_1)}, \quad \epsilon_d = \frac{f(P_1) - f(P_0)}{f(P_0)}
$$
che chiamiamo rispetivamente \textbf{errore relativo algoritmico} e \textbf{errore relativo inerente}.
Questo viene da:
$$
\epsilon_f = [...] = \frac{f_a(P_1) - f(P_1)}{f(P_1)} \cdot \frac{f(P_1)}{f(P_0)} + \frac{f(P_1) - f(P_0)}{f(P_0)} 
$$
si nota che $\frac{f(P_1)}{f(P_0)} = 1 + \frac{f(P_1) - f(P_0)}{f(P_0)}$, e quindi:
$$
= \epsilon_a \cdot \left( 1 + \frac{f(P_1) - f(P_0)}{f(P_0)} \right) + \epsilon_d = \epsilon_a(1 + \epsilon_d) + \epsilon_d = \epsilon_a + \epsilon_d + \epsilon_a \epsilon_d \approx \epsilon_a + \epsilon_d
$$
assumendo $\epsilon_a \epsilon_d \approx 0$.

\par\smallskip

Ci interessa dare stime superiori per i valori assoluti di errori assoluti e relativi, come avevamo fatto per gli errori delle funzioni di arrotondamento da reali a numeri macchina.
In generale, quindi, per limitare $|\sigma_f|$ cercheremo disuguaglianze $|\sigma_a| < \tau_1$, $|\sigma_d| < \tau_2$, da cui:
$$
	|\sigma_f| < \tau_1 + \tau_2
$$

\subsubsection{Stima dell'errore inerente}
Avevamo quindi definito l'\textbf{errore assoluto inerente}:
$$
\sigma_d = f(P_1) - f(P_0)
$$

Sotto l'ipotesi $f \in C^1(D)$ per $D \subset \mathbb{R}^m$ che contiene $P_0$, si può usare  lo sviluppo di Taylor di $f$ in $P_0$, troncato al primo ordine:
$$
f(P_1) - f(P_0) = f(P_0) + \nabla f(\overline{P})^T (P_1 - P_0) - f(P_0) = \nabla f(\overline{P})^T (P_1 - P_0)
$$
$$
= \sum_{j=1}^m \frac{\partial f}{\partial x_j}(\overline{P}) \cdot \left(x_j^{(1)} - x_j^{(0)}\right)
\approx \sum_{j=1}^m \frac{\partial f}{\partial x_j}{P_0} \cdot \left(x_j^{(1)} - x_j^{(0)}\right)
$$
dove $\overline{P}$ è un punto che sta sul segmento $\overline{P_1 P_0}$.
Da questo potremo dire:
$$
\sigma_d = \sum_{j=1}^m \frac{\partial f}{\partial x_j}{P_0} \cdot \sigma_j
$$
dove $\sigma_j = \left(x_j^{(1)} - x_j^{(0)}\right)$ è l'\textbf{errore di arrotondemento} nella componente $j$ di $P_0$, e $\frac{\partial f}{\partial x_j}{P_0}$ viene detto \textbf{coefficiente di amplificazione}.

\par\smallskip

Per l'\textbf{errore relativo inerente}, che avevamo definito come:
$$
\epsilon_d = \frac{f(P_1) - f(P_0)}{f(P_0)}
$$
potremo fare considerazioni simili: 
$$
\epsilon_d = \frac{ \sum_{j=1}^{m} \frac{\partial f}{\partial x_j} (P_0) \cdot \sigma_j }{f(P_0)} = \sum_{j=1}^m \frac{ x_j^{(1)} - x_j^{(0)} }{x_j^{(0)}} \cdot \frac{\partial f}{\partial x_j} (P_0) \cdot \frac{x_j^{(0)}}{f(P_0)}  
$$
dove $\epsilon_j = \frac{ x_j^{(1)} - x_j^{(0)} }{x_j^{(0)}}$ sarà l'\textbf{errore di arrotondamento relativo} nella componente $j$ di $P_0$ e $P_j = \frac{\partial f}{\partial x_j} (P_0) \cdot \frac{x_j^{(0)}}{f(P_0)}$ viene detto \textbf{coefficiente di amplificazione dell'errore relativo}.

La formula finale sarà quindi:
$$
\epsilon_d = \sum_{j=1}^m \epsilon_j P_j
$$

I problemi in cui si devono calcolare $f$ i cui coefficienti di amplificazione degli errori relativi sono grandi in modulo (o ce n'è almeno uno sufficientemente grande) si dicono \textbf{malcondizionati}.
Viceversa, se $|P_j|$ è vicino a $1$ per ogni $j$ il problema si dice \textbf{ben condizionato}, cioè che $\epsilon_d$ è di un ordine di grandezza comparabile a $\max(\epsilon_i)$

Notiamo inoltre che il condizionamento di un problema dipende solamente dalla sua struttura matematica. 

\subsubsection{Errori inerenti delle operazioni aritmetiche}
Vediamo gli errori inerenti associati alle 4 operazioni aritmetiche $+, -, \times, \div$:
\begin{table}[H]
	\center \rowcolors{2}{white}{black!10}
	\begin{tabular} { c | c | c }
		\bfseries Operazione & $\sigma_d$ & $\epsilon_d$ \\
		\hline
		$x \oplus y$ & $\sigma_x + \sigma_y$ & $\frac{x}{x + y} \epsilon_x + \frac{y}{x + y} \epsilon_y$ \\ 
		$x \ominus y$ & $\sigma_x - \sigma_y$ & $\frac{x}{x - y} \epsilon_x - \frac{y}{x - y} \epsilon_y$ \\ 
		$x \otimes y$ & $y\sigma_x + x\sigma_y$ & $\epsilon_x + \epsilon_y$ \\ 
		$x \oslash y$ & $\frac{1}{y}\sigma_x - \frac{x}{y^2}\sigma_y$ & $\epsilon_x - \epsilon_y$ \\ 
	\end{tabular}
\end{table}

Notiamo come somme e sottrazioni non amplificano gli errori totali, mentre prodotti e divisioni non amplificano gli errori relativi (riguardo agli errori inerenti).
Questo significa che somme e sottrazioni possono avere errori relativi grandi quando $|x + y| << \min \{ |x|, |y|\}$. 
Questo effetto viene detto \textbf{cancellazione numerica}.

\subsubsection{Stima dell'errore algoritmico} 
Avevamo definito un algoritmo $f_a(x)$ di cui vogliamo stimare l'errore algoritmico assoluto $\sigma_a = f_a(P_1) - f(P_1)$.
Assumiamo $P_1 = Rn(P_0) \in F(\beta, m, L, U)$, cioè gli operandi come privi di errori iniziali di arrotondamento. 

L'idea è di seguire l'errore generato dall'algoritmo sul grafo (o albero) che lo rappresenta sfruttando le relazioni per l'errore inerente nelle 4 operazioni aritmetiche.
Prendiamo ad esempio la funzione:
$$
f(x, y, z, w) = x \cdot \left(\frac{y}{z} - w\right)
$$
Avremo i risultati intermedi:
\begin{itemize}
	\item $r_1 = \frac{y}{z}$
	\item $r_2 = r_1 - w$
	\item $r_3 = r_2 \cdot x$
\end{itemize}
Di cui riportiamo il grafo:
\begin{center}
	\begin{forest}
		[{$r_3 = r_2 \cdot x$}
			[$x$]
			[{$r_2 = r_1 - w$}
				[$w$]
				[{$r_1 = \frac{y}{z}$}
					[$y$]
					[$z$]
				]
			]
		]	
	\end{forest}
\end{center}
	dove $\epsilon_3$, $\epsilon_2$ e $\epsilon_1$ saranno termini associati ad ogni risultato intermedio che rappresenteranno gli errori di troncamento dei risultati intermedi stessi, e $\epsilon_{r3}$, $\epsilon_{r2}$ e $\epsilon_{r1}$ rappresenteranno gli errori inerenti delle singole operazioni per il calcolo dei risultati intermedi.

Partiamo dalla radice per valutare gli errori:
$$
\epsilon_d = \epsilon_{r_3} = \epsilon_3 + \epsilon_x + \epsilon_{r_2} = \epsilon_3 + \epsilon_{r_2} 
$$
$$
= \epsilon_3 + \epsilon_2 + \frac{-zw}{y-zw} \cdot \epsilon_w + \frac{y}{y-zw}\cdot\epsilon_{r_1} = \epsilon_3 + \epsilon_2 + \frac{y}{y - zw} \cdot \epsilon_{r_1}
$$
$$
= \epsilon_3 + \epsilon_2 + \frac{y}{y-zw}\left( \epsilon_1 + \epsilon_y - \epsilon_z \right) = \epsilon_3 + \epsilon_2 + \frac{y}{y-zw}\cdot \epsilon_1 = \epsilon_a
$$

Dove abbiamo ignorato i termini di errore relativo $\epsilon_i$ legati ad ogni variabile (come avevamo detto sopra, gli operandi sono considerati come privi di errore di arrotondamento). 
Per la stima di $\epsilon_3$, $\epsilon_2$ e $\epsilon_1$, vale $\epsilon_i \leq U$ precisione macchina.
Nel caso di errori assoluti vale $|\sigma_i| \leq U \cdot \max(x_i)$ considerata ogni variabile $x_i$.

\par\smallskip

Chiaramente, diversi algoritmi equivalenti in aritmetica esatta potranno avere errori algoritmici diversi fatte tutte le approssimazioni.

Prendiamo ad esempio la funzione $f(x, y) = x^2 - y^2$.
Potremmo adottare due algoritmi:
\begin{enumerate}
	\item Si prendono i risultati intermedi:
		$$
			r_1 = x \cdot x
		$$
		$$
			r_2 = y \cdot y
		$$
		$$
			r_3 = x - y
		$$
		Da cui il grafo:
		\begin{center}
			\begin{forest}
				[
					{$r_3 = r_1 - r_2$}
					[{$r_1 = x \cdot x$}
						[$x$]
						[$x$]
					]
					[{$r_2 = y \cdot y$}
						[$y$]
						[$y$]
					]
				]	
			\end{forest}
		\end{center}
		Questo approccio potrebbe risultare il più intuitivo: dalla stima dell'errore si ha:
		$$
		\epsilon_{r_3} = \epsilon_1 + \frac{x^2}{x^2 - y^2}\epsilon_{r_1} - \frac{y^2}{x^2 - y^2}\epsilon_{r_2} = \epsilon_1 + \frac{x^2}{x^2 - y^2}(\epsilon_1 + \epsilon_x + \epsilon_x) - \frac{y^2}{x^2 - y^2}(\epsilon_2 + \epsilon_y + \epsilon_y)
		$$
		$$
= \epsilon_1 + \frac{x^2}{x^2 - y^2}\epsilon_1 - \frac{y^2}{x^2 - y^2}\epsilon_2 
		$$
	
	\item Un altro approccio è quello di prendere i risultati intermedi:
		$$
			r_1 = x + y
		$$
		$$
			r_2 = x - y
		$$
		$$
			r_3 = r_1 \cdot r_2
		$$
		Da cui il grafo:
		\begin{center}
			\begin{forest}
				[
					{$r_3 = r_1 \cdot r_2$}
					[{$r_1 = x + y$}
						[$x$]
						[$y$]
					]
					[{$r_2 = x - y$}
						[$x$]
						[$y$]
					]
				]	
			\end{forest}
		\end{center}
		Vediamo la stima dell'errore:
		$$
		\epsilon_{r_3} = \epsilon_3 + \epsilon_{r_1} + \epsilon_{r_2} = \epsilon_3 + \epsilon_1 + \frac{x}{x + y} \epsilon_x + \frac{y}{x + y} \epsilon_y + \epsilon_2 + \frac{x}{x - y} \epsilon_x - \frac{y}{x - y} \epsilon_y 
		$$
		$$
 		= \epsilon_3 + \epsilon_1 + \epsilon_2
		$$
\end{enumerate}
Notiamo che la stima dell'errore del secondo approccio è più conveniente, in quanto più strettamente limitata al di sotto di un valore fisso: $\epsilon_1 + \epsilon_2 + \epsilon_3 = 3U$, contro il $\left( 1 + \frac{|x^2 + y^2|}{|x^2 - y^2|} \right) U$ del primo approccio.

\par\smallskip

Abbiamo visto quindi tenciche per la stima di $\epsilon_a$ e $\epsilon_d$ ($\sigma_a$ e $\sigma_d$), che ci permettono di calcolare $|\epsilon_f| \leq |\epsilon_a| + |\epsilon_d|$ ($|\sigma_f| \leq |\sigma_a| + |\sigma_d|$). 

\subsubsection{Problema diretto}
Un problema classico sarà quello di, data $f$, un algoritmo risolutivo $f_a$ e una stima degli errori $d_{x_i}$, di stimare $\sigma_f$ per $P_0 \in D \subset \mathbb{R}^m$.

Ad esempio, prendiamo la funzione:
$$
f : D \rightarrow \mathbb{R}, \quad f(x_1, x_2) = \frac{x_1}{x_2}, \quad D = [1, 3] \times [4, 5]
$$
Diamo una stima dell'errore di arrotondamento delle variabili:
$$
|\sigma_{x_i}| \leq \frac{1}{2} \cdot 10^{-2}
$$

\begin{itemize}
	\item
Iniziamo con la stima dell'errore inerente.
Si ha, dall'errore assoluto inerente della divisione:
$$
\sigma_d = \frac{1}{x_2} \sigma_{x_1} - \frac{x_1}{x_2^2} \sigma_{x_2} 
$$

Quello che ci interessa è dare stime superiori, quindi prendiamo i due valori:
$$
\left| \frac{1}{x_2} \right| \approx \frac{1}{4}, \quad
\left| \frac{x_1}{x_2^2} \right| \approx \frac{3}{16}
$$
Dai valori massimi che si possono ottenere sul dominio $D$, da cui:
$$
|\sigma_d| \leq \frac{1}{4} \cdot \frac{1}{2} \cdot 10^{-2} + \frac{3}{16} \cdot \frac{1}{2} \cdot 10^{-2} = \left( \frac{1}{8} + \frac{7}{32} \right) \cdot 10^{-2} = \frac{7}{32} \cdot 10^{-2} 
$$

	\item
Vediamo quindi la stima dell'errore algoritmico.
L'albero dell'algoritmo sarà banalmente:
\begin{center}
	\begin{forest}
		[
			{$r_1 = \frac{x_1}{x_2}$}
			[{$x_1$}]
			[{$x_2$}]
		]	
	\end{forest}
\end{center}
da cui posto $\sigma_{x_i} = 0$ resterà solo l'errore di arrotondamento assoluto $\sigma_1$:
$$
|\sigma_a| = \sigma_1 = U \cdot \max\left( \frac{x_1}{x_2} \right) = \frac{3}{4} \cdot \frac{1}{2} \cdot 10^{-2} = \frac{3}{8} \cdot 10^{-2}
$$
\end{itemize}

Otteniamo quindi l'errore totale come la somma dell'errore algoritmico e dell'errore inerente, cioè:
$$
|\sigma_f| = |\sigma_a| + |\sigma_f| = \left( \frac{7}{32} + \frac{3}{8} \right) \cdot 10^{-2} = \frac{19}{32} \cdot 10^{-2}
$$

\subsubsection{Problema inverso}
Il problema inverso potrebbe essere quello di, dato $\tau > 0$, $f$ e un punto $P_0 \in \mathbb{R}^n$, determinare un algoritmo ed un valore di precisione macchina $U$ tali per cui $|s_f| < \tau$.

Prendiamo ad esempio il problema considerato prima, ma sul dominio:
$$
D = [1, 2] \times [-2, -1]
$$
e cerchiamo una precisione macchina $U$ tale per cui l'errore assoluto $\sigma_f$ è limitato a $\tau = 10^{-2}$, cioè:
$$
|\sigma_f| \leq 10^{-2}
$$

\begin{itemize}
	\item Iniziamo di nuovo con la stima dell'errore inerente. Dagli stessi errori inerenti considerati prima si hanno le stime:
	$$
	\left| \frac{1}{x_2} \right| \approx 1, \quad
	\left| \frac{x_1}{x_2^2} \right| \approx 2
	$$
	Dai valori massimi che si possono ottenere sul dominio $D$, da cui:
	$$
	|\sigma_d| \leq U + 2U = 3U 
	$$
\item Vediamo quindi la stima dell'errore algoritmico, che prendendo il valore massimo della funzione in maniera analoga a prima sarà:
$$
|\sigma_a| = \sigma_1 = U \cdot \max\left( \frac{x_1}{x_2} \right) = 2U 
$$
L'errore totale sarà quindi:
$|\sigma_f| = |\sigma_a| + |\sigma_f| = 3U + 2U = 5U$ 

Imponiamo quindi la diseguaglianza iniziale, cioè:
$$
|\sigma_f| = 5U \leq 10^{-2} \implies U \leq \frac{1}{5} \cdot 10^{-2}
$$
\end{itemize}

\end{document}
